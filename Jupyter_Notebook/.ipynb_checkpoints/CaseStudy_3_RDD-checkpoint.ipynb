{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Processing\n",
    "--------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "// Preparing the Dataset for training and testing of models\n",
    "\n",
    "import org.apache.spark.mllib.classification.{NaiveBayes, NaiveBayesModel}\n",
    "import org.apache.spark.mllib.util.MLUtils\n",
    "import org.apache.spark.mllib.evaluation.MulticlassMetrics\n",
    "import org.apache.spark.mllib.evaluation.BinaryClassificationMetrics\n",
    "\n",
    "// Load and parse the data file.\n",
    "val data = MLUtils.loadLibSVMFile(sc, \"libsvm.data\")\n",
    "\n",
    "// Split data into training (70%) and test (30%).\n",
    "val Array(training, test) = data.randomSplit(Array(0.7, 0.3),seed=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes Algorithm\n",
    "-------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix of Naive Bayes is\n",
      "2873.0  4579.0  \n",
      "15.0    316.0   \n",
      "\n",
      "The Accuracy of Naive Bayes is\n",
      "0.409739175125273\n",
      "\n",
      "The ROC for Naive Bayes is\n",
      "(0.0,0.0)(0.6144659151905528,0.9546827794561934)(1.0,1.0)(1.0,1.0)()\n",
      "The Area under ROC for Naive Bayes is\n",
      "0.6701084321328203\n",
      "\n",
      "The Precision of Naive Bayes is\n",
      "0.409739175125273\n",
      "\n",
      "The Precision by Threshold for Naive Bayes is\n",
      "(1.0,0.06455566905005107)(0.0,0.04252858794809199)\n",
      "Execution Time for Training and Prediction of Naive Bayes is\n",
      "438ms\n"
     ]
    }
   ],
   "source": [
    "\n",
    "val startTime_dt=System.currentTimeMillis\n",
    "\n",
    "val model = NaiveBayes.train(training, lambda = 1.0, modelType = \"multinomial\")\n",
    "\n",
    "val predictionAndLabels = test.map(p => (model.predict(p.features), p.label))\n",
    "\n",
    "val endTime_dt=System.currentTimeMillis\n",
    "\n",
    "// Get evaluation metrics.\n",
    "val bMetrics_dt = new BinaryClassificationMetrics(predictionAndLabels)\n",
    "val metrics_dt=new MulticlassMetrics(predictionAndLabels)\n",
    "\n",
    "\n",
    "//Model Evaluation\n",
    "println(\"Confusion Matrix of Naive Bayes is\")\n",
    "println(metrics_dt.confusionMatrix)\n",
    "println(\"\\nThe Accuracy of Naive Bayes is\")\n",
    "println(metrics_dt.accuracy)\n",
    "println(\"\\nThe ROC for Naive Bayes is\")\n",
    "bMetrics_dt.roc.collect().foreach(print)\n",
    "println(\"\\n\")\n",
    "println(\"\\nThe Area under ROC for Naive Bayes is\")\n",
    "println(bMetrics_dt.areaUnderROC)\n",
    "println(\"\\nThe Precision of Naive Bayes is\")\n",
    "println(metrics_dt.precision)\n",
    "println(\"\\nThe Precision by Threshold for Naive Bayes is\")\n",
    "bMetrics_dt.precisionByThreshold.collect().foreach(print)\n",
    "print(\"\\n\")\n",
    "println(\"\\nExecution Time for Training and Prediction of Naive Bayes is\")\n",
    "println(endTime_dt-startTime_dt + \"ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we started our analysis with basic model,Naive bayes theorem which determines probability of future events based on earlier frequency. This model outperformed all other models in terms of speed but didn't perform well in terms of accuracy,prescision for the given dataset.Since it uses probablity based prediction and treats each record as independent ,it was handling imbalanced data inherently, the values of 'Precision by Threshold is the proof for this. To achieve better accuracy rate we planned of exploring other RDD based ML models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear Support Vector Machines (SVM):\n",
    "-------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix of Linear SVM L1 regularization is\n",
      "6163.0  1289.0  \n",
      "235.0   96.0    \n",
      "\n",
      "The Accuracy of Linear SVM L1 regularization is\n",
      "0.8041886162148272\n",
      "\n",
      "The ROC for Linear SVM L1 regularization is\n",
      "(0.0,0.0)(0.17297369833601717,0.29003021148036257)(1.0,1.0)(1.0,1.0)()\n",
      "\n",
      "The Area under ROC for Linear SVM L1 regularization is\n",
      "0.5585282565721728\n",
      "\n",
      "The Precision of Linear SVM L1 regularization is\n",
      "0.8041886162148272\n",
      "\n",
      "The Precision by Threshold for Linear SVM L1 regularization is\n",
      "(1.0,0.06931407942238267)(0.0,0.04252858794809199)()\n",
      "\n",
      "Execution Time for Training and Prediction of Linear SVM L1 regularization is\n",
      "2735ms\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.mllib.classification.{SVMModel, SVMWithSGD}\n",
    "import org.apache.spark.mllib.evaluation.BinaryClassificationMetrics\n",
    "import org.apache.spark.mllib.optimization.L1Updater\n",
    "import org.apache.spark.mllib.evaluation.MulticlassMetrics\n",
    "\n",
    "val svmAlg = new SVMWithSGD()\n",
    "svmAlg.optimizer.setNumIterations(200).setRegParam(0.1).setUpdater(new L1Updater)\n",
    "\n",
    "val startTime_dt=System.currentTimeMillis\n",
    "\n",
    "val modelL1 = svmAlg.run(training)\n",
    "\n",
    "val scoreAndLabels = test.map { point =>\n",
    "  val score = modelL1.predict(point.features)\n",
    "  (score, point.label)\n",
    "}\n",
    "\n",
    "val endTime_dt=System.currentTimeMillis\n",
    "\n",
    "// Get evaluation metrics.\n",
    "val bMetrics_dt = new BinaryClassificationMetrics(scoreAndLabels)\n",
    "val metrics_dt=new MulticlassMetrics(scoreAndLabels)\n",
    "\n",
    "\n",
    "//Model Evaluation\n",
    "println(\"Confusion Matrix of Linear SVM L1 regularization is\")\n",
    "println(metrics_dt.confusionMatrix)\n",
    "println(\"\\nThe Accuracy of Linear SVM L1 regularization is\")\n",
    "println(metrics_dt.accuracy)\n",
    "println(\"\\nThe ROC for Linear SVM L1 regularization is\")\n",
    "println(bMetrics_dt.roc.collect().foreach(print))\n",
    "print(\"\\n\")\n",
    "println(\"\\nThe Area under ROC for Linear SVM L1 regularization is\")\n",
    "println(bMetrics_dt.areaUnderROC)\n",
    "println(\"\\nThe Precision of Linear SVM L1 regularization is\")\n",
    "println(metrics_dt.precision)\n",
    "println(\"\\nThe Precision by Threshold for Linear SVM L1 regularization is\")\n",
    "println(bMetrics_dt.precisionByThreshold.collect().foreach(print))\n",
    "println(\"\\nExecution Time for Training and Prediction of Linear SVM L1 regularization is\")\n",
    "println(endTime_dt-startTime_dt + \"ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above method performs L1 regularization with the regularization parameter set to 0.1 which adds “absolute value of magnitude” of coefficient as penalty term to the loss function and also it shrinks the less important feature’s coefficient to zero thus, removing some feature altogether. Just like Naive Bayes, SVM handled the imbalanced data very well and hence the 'Precision by Threshold' looks almost balanced. On repeatative executions we observed many irregularities in the performance of Linear SVM L1 regularization but the accuracy is good compared to Naive Bayes Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimizing the Linear SVM Model:\n",
    "-----------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix of Linear SVM L2 regularization is\n",
      "7452.0  0.0  \n",
      "331.0   0.0  \n",
      "\n",
      "The Accuracy of Linear SVM L2 regularization is\n",
      "0.957471412051908\n",
      "\n",
      "The ROC for Linear SVM L2 regularization is\n",
      "(0.0,0.0)(1.0,1.0)(1.0,1.0)()\n",
      "\n",
      "The Area under ROC for Linear SVM L2 regularization is\n",
      "0.5\n",
      "\n",
      "The Precision of Linear SVM L2 regularization is\n",
      "0.957471412051908\n",
      "\n",
      "The Precision by Threshold for Linear SVM L2 regularization is\n",
      "(0.0,0.04252858794809199)()\n",
      "\n",
      "Execution Time for Training and Prediction of Linear SVM L2 regularization is\n",
      "1831ms\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.mllib.classification.{SVMModel, SVMWithSGD}\n",
    "import org.apache.spark.mllib.evaluation.BinaryClassificationMetrics\n",
    "import org.apache.spark.mllib.util.MLUtils\n",
    "import org.apache.spark.mllib.evaluation.MulticlassMetrics\n",
    "\n",
    "// Run training algorithm to build the model\n",
    "val numIterations = 100\n",
    "\n",
    "val startTime_dt=System.currentTimeMillis\n",
    "\n",
    "val model = SVMWithSGD.train(training, numIterations)\n",
    "\n",
    "\n",
    "// Compute raw scores on the test set.\n",
    "val scoreAndLabels = test.map { point =>\n",
    "  val score = model.predict(point.features)\n",
    "  (score, point.label)\n",
    "}\n",
    "val endTime_dt=System.currentTimeMillis\n",
    "\n",
    "// Get evaluation metrics.\n",
    "val bMetrics_dt = new BinaryClassificationMetrics(scoreAndLabels)\n",
    "val metrics_dt=new MulticlassMetrics(scoreAndLabels)\n",
    "\n",
    "\n",
    "//Model Evaluation\n",
    "println(\"Confusion Matrix of Linear SVM L2 regularization is\")\n",
    "println(metrics_dt.confusionMatrix)\n",
    "println(\"\\nThe Accuracy of Linear SVM L2 regularization is\")\n",
    "println(metrics_dt.accuracy)\n",
    "println(\"\\nThe ROC for Linear SVM L2 regularization is\")\n",
    "println(bMetrics_dt.roc.collect().foreach(print))\n",
    "print(\"\\n\")\n",
    "println(\"\\nThe Area under ROC for Linear SVM L2 regularization is\")\n",
    "println(bMetrics_dt.areaUnderROC)\n",
    "println(\"\\nThe Precision of Linear SVM L2 regularization is\")\n",
    "println(metrics_dt.precision)\n",
    "println(\"\\nThe Precision by Threshold for Linear SVM L2 regularization is\")\n",
    "println(bMetrics_dt.precisionByThreshold.collect().foreach(print))\n",
    "println(\"\\nExecution Time for Training and Prediction of Linear SVM L2 regularization is\")\n",
    "println(endTime_dt-startTime_dt + \"ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To optimize the Linear SVM with L1 regularization we trained a Linear Support Vector Machine (SVM) using Stochastic Gradient Descent using L2 regularization with the regularization parameter set to 1.0. It adds “squared magnitude” of coefficient as penalty term to the loss function and also this technique works very well to give better performnace .Applying the L2 regularization which uses the squared magnitude improved the performance of the model from 80 to 95 percentile. But we cannot fully justify this improvement in accuracy as we are not seeing a balanced 'Precision by Threshold' for some reason. It could be because of over-fitting or because of the imbalanced data. We also observed a reduction in execution time compared to L1 regularisation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression\n",
    "-------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix of Logistic regression L1 regularization is\n",
      "7452.0  0.0  \n",
      "331.0   0.0  \n",
      "\n",
      "The Accuracy of Logistic regression L1 regularization is\n",
      "0.957471412051908\n",
      "\n",
      "The ROC for Logistic regression L1 regularization is\n",
      "(0.0,0.0)(1.0,1.0)(1.0,1.0)\n",
      "The Area under ROC for Logistic regression L1 regularization is\n",
      "0.5\n",
      "\n",
      "The Precision of Logistic regression L1 regularization is\n",
      "0.957471412051908\n",
      "\n",
      "The Precision by Threshold for Logistic regression L1 regularization is\n",
      "(0.0,0.04252858794809199)()\n",
      "\n",
      "Execution Time for Training and Prediction of Logistic regression L1 regularization is\n",
      "1795ms\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.mllib.classification.{LogisticRegressionModel, LogisticRegressionWithLBFGS,LogisticRegressionWithSGD}\n",
    "import org.apache.spark.mllib.evaluation.MulticlassMetrics\n",
    "import org.apache.spark.mllib.evaluation.BinaryClassificationMetrics\n",
    "import org.apache.spark.mllib.regression.LabeledPoint\n",
    "import org.apache.spark.mllib.util.MLUtils\n",
    "\n",
    "val startTime_dt=System.currentTimeMillis\n",
    "\n",
    "// Run training algorithm to build the model\n",
    "val model = new LogisticRegressionWithSGD().run(training)\n",
    "\n",
    "// Compute raw scores on the test set.\n",
    "val predictionAndLabels = test.map { case LabeledPoint(label, features) =>\n",
    "  val prediction = model.predict(features)\n",
    "  (prediction, label)\n",
    "}\n",
    "\n",
    "val endTime_dt=System.currentTimeMillis\n",
    "\n",
    "// Get evaluation metrics.\n",
    "val bMetrics_dt = new BinaryClassificationMetrics(predictionAndLabels)\n",
    "val metrics_dt=new MulticlassMetrics(predictionAndLabels)\n",
    "\n",
    "\n",
    "//Model Evaluation\n",
    "println(\"Confusion Matrix of Logistic regression L1 regularization is\")\n",
    "println(metrics_dt.confusionMatrix)\n",
    "println(\"\\nThe Accuracy of Logistic regression L1 regularization is\")\n",
    "println(metrics_dt.accuracy)\n",
    "println(\"\\nThe ROC for Logistic regression L1 regularization is\")\n",
    "bMetrics_dt.roc.collect().foreach(print)\n",
    "print(\"\\n\")\n",
    "println(\"\\nThe Area under ROC for Logistic regression L1 regularization is\")\n",
    "println(bMetrics_dt.areaUnderROC)\n",
    "println(\"\\nThe Precision of Logistic regression L1 regularization is\")\n",
    "println(metrics_dt.precision)\n",
    "println(\"\\nThe Precision by Threshold for Logistic regression L1 regularization is\")\n",
    "println(bMetrics_dt.precisionByThreshold.collect().foreach(print))\n",
    "println(\"\\nExecution Time for Training and Prediction of Logistic regression L1 regularization is\")\n",
    "println(endTime_dt-startTime_dt + \"ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above method performs L1 regularization with the regularization parameter set to 0.1 which adds “absolute value of magnitude” of coefficient as penalty term to the loss function and also it shrinks the less important feature’s coefficient to zero. We observed many irregularities in the performance of Logistic regression with L1 regularization.Especially with accuracy ,ROC and Precision by Threshold. Detailed study with repeatative execution helps us in deriving the conclusion that the model is not handling imbalanced data and is over-fitting. Logistic Regression - Data Frame model had weight Parameter to handle imbalanced data but for RDDs there is no such option available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimization of Logistic regression:\n",
    "-------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix of Logistic regression L2 regularization is\n",
      "7382.0  70.0   \n",
      "135.0   196.0  \n",
      "\n",
      "The Accuracy of Logistic regression L2 regularization is\n",
      "0.9736605422073751\n",
      "\n",
      "The ROC for Logistic regression L2 regularization is\n",
      "(0.0,0.0)(0.00939345142243693,0.5921450151057401)(1.0,1.0)(1.0,1.0)()\n",
      "\n",
      "The Area under ROC for Logistic regression L2 regularization is\n",
      "0.7913757818416516\n",
      "\n",
      "The Precision of Logistic regression L2 regularization is\n",
      "0.9736605422073751\n",
      "\n",
      "The Precision by Threshold for Logistic regression L2 regularization is\n",
      "(1.0,0.7368421052631579)(0.0,0.04252858794809199)()\n",
      "\n",
      "The Model Execution Time for Logistic regression L2 regularization is\n",
      "247ms\n"
     ]
    }
   ],
   "source": [
    "\n",
    "val startTime_dt=System.currentTimeMillis\n",
    "val model = new LogisticRegressionWithLBFGS().setNumClasses(10).run(training)\n",
    "\n",
    "val startTime_dt=System.currentTimeMillis\n",
    "\n",
    "val predictionAndLabels = test.map { case LabeledPoint(label, features) =>\n",
    "  val prediction = model.predict(features)\n",
    "  (prediction, label)\n",
    "}\n",
    "\n",
    "val endTime_dt=System.currentTimeMillis\n",
    "\n",
    "// Get evaluation metrics.\n",
    "val bMetrics_dt = new BinaryClassificationMetrics(predictionAndLabels)\n",
    "val metrics_dt=new MulticlassMetrics(predictionAndLabels)\n",
    "\n",
    "\n",
    "//Model Evaluation\n",
    "println(\"Confusion Matrix of Logistic regression L2 regularization is\")\n",
    "println(metrics_dt.confusionMatrix)\n",
    "println(\"\\nThe Accuracy of Logistic regression L2 regularization is\")\n",
    "println(metrics_dt.accuracy)\n",
    "println(\"\\nThe ROC for Logistic regression L2 regularization is\")\n",
    "println(bMetrics_dt.roc.collect().foreach(print))\n",
    "print(\"\\n\")\n",
    "println(\"\\nThe Area under ROC for Logistic regression L2 regularization is\")\n",
    "println(bMetrics_dt.areaUnderROC)\n",
    "println(\"\\nThe Precision of Logistic regression L2 regularization is\")\n",
    "println(metrics_dt.precision)\n",
    "println(\"\\nThe Precision by Threshold for Logistic regression L2 regularization is\")\n",
    "println(bMetrics_dt.precisionByThreshold.collect().foreach(print))\n",
    "println(\"\\nThe Model Execution Time for Logistic regression L2 regularization is\")\n",
    "println(endTime_dt-startTime_dt + \"ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Logistic Regression, L-BFGS version is strongly recommended since it converges faster and more accurately compared to SGD by approximating the inverse Hessian matrix using quasi-Newton method. Applying the L2 regularization which uses the squared magnitude improved the performance of the model and it is evident while looking at confusion matrix, accuracy, ROC. It handled imbalanced data also reasonably well, somewhat balanced Precision by Threshold is the proof for this statement. The Execution time also was found to be very less which gives all the signs of a good reliable model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Tree    \n",
    "------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix of Decision Tree is\n",
      "7423.0  29.0   \n",
      "29.0    302.0  \n",
      "\n",
      "The Accuracy of Decision Tree is\n",
      "0.9925478607220866\n",
      "\n",
      "The ROC for Decision Tree is\n",
      "(0.0,0.0)(0.0038915727321524422,0.9123867069486404)(1.0,1.0)(1.0,1.0)()\n",
      "\n",
      "The Area under ROC for Decision Tree is\n",
      "0.954247567108244\n",
      "\n",
      "The Precision of Decision Tree is\n",
      "0.9925478607220866\n",
      "\n",
      "The Precision by Threshold for Decision Tree is\n",
      "(1.0,0.9123867069486404)(0.0,0.04252858794809199)()\n",
      "\n",
      "Execution Time for Training and Prediction of Decision Tree is\n",
      "1481ms\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.mllib.tree.DecisionTree\n",
    "import org.apache.spark.mllib.tree.model.DecisionTreeModel\n",
    "import org.apache.spark.mllib.util.MLUtils\n",
    "\n",
    "// Train a DecisionTree model.\n",
    "val numClasses = 2\n",
    "val categoricalFeaturesInfo = Map[Int, Int]()\n",
    "val impurity = \"gini\"\n",
    "val maxDepth = 10\n",
    "val maxBins = 32\n",
    "\n",
    "val startTime_dt=System.currentTimeMillis\n",
    "\n",
    "val model = DecisionTree.trainClassifier(training, numClasses, categoricalFeaturesInfo,\n",
    "  impurity, maxDepth, maxBins)\n",
    "\n",
    "// Evaluate model on test instances and compute test error\n",
    "val labelAndPreds = test.map { point =>\n",
    "  val prediction = model.predict(point.features)\n",
    "  (point.label, prediction)\n",
    "}\n",
    "\n",
    "val endTime_dt=System.currentTimeMillis\n",
    "\n",
    "//Model Evaluation\n",
    "val metrics_dt = new MulticlassMetrics(labelAndPreds)\n",
    "val bMetrics_dt = new BinaryClassificationMetrics(labelAndPreds)\n",
    "println(\"Confusion Matrix of Decision Tree is\")\n",
    "println(metrics_dt.confusionMatrix)\n",
    "println(\"\\nThe Accuracy of Decision Tree is\")\n",
    "println(metrics_dt.accuracy)\n",
    "println(\"\\nThe ROC for Decision Tree is\")\n",
    "println(bMetrics_dt.roc.collect().foreach(print))\n",
    "print(\"\\n\")\n",
    "println(\"\\nThe Area under ROC for Decision Tree is\")\n",
    "println(bMetrics_dt.areaUnderROC)\n",
    "println(\"\\nThe Precision of Decision Tree is\")\n",
    "println(metrics_dt.precision)\n",
    "println(\"\\nThe Precision by Threshold for Decision Tree is\")\n",
    "println(bMetrics_dt.precisionByThreshold.collect().foreach(print))\n",
    "println(\"\\nExecution Time for Training and Prediction of Decision Tree is\")\n",
    "println(endTime_dt-startTime_dt + \"ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Trees are known for over-fitting the data and the the results above support that statement especially the accuracy rate of 99.2%. This is mainly because of that fact that decision Tree does not incorporate bootstrapping or any other technique to understand complete trend in the data set.It takes a random column as the root and then start building the decision tree from there. This is one reason for over-fitting. The behaviour was observed to be very similar to the data frame based model. It was not handling imbalanced data too. The execution time for RDD based decision tree is considerably less than it's dataframe version. We handled imbalanced data by setting threshold in the data frame based model. There is no such option available in RDD decision tree. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest\n",
    "------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix of Random Forest is\n",
      "7447.0  27.0                                                                    \n",
      "5.0     304.0  \n",
      "\n",
      "The Accuracy of Random Forest is\n",
      "0.9958884748811512                                                              \n",
      "\n",
      "The ROC for Random Forest is\n",
      "(0.0,0.0)(0.0036125234145036127,0.9838187702265372)(1.0,1.0)(1.0,1.0)()\n",
      "\n",
      "The Area under ROC for Random Forest is\n",
      "0.9901031234060168\n",
      "\n",
      "The Precision of Random Forest is\n",
      "0.9958884748811512\n",
      "\n",
      "The Precision by Threshold for Random Forest is\n",
      "(1.0,0.918429003021148)(0.0,0.03970191442888346)()\n",
      "\n",
      "Execution Time for Training and Prediction of Random Forest is\n",
      "70520ms\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.mllib.tree.RandomForest\n",
    "import org.apache.spark.mllib.tree.model.RandomForestModel\n",
    "import org.apache.spark.mllib.util.MLUtils\n",
    "\n",
    "// Train a RandomForest model.\n",
    "val numClasses = 2\n",
    "val categoricalFeaturesInfo = Map[Int, Int]()\n",
    "val numTrees = 500\n",
    "val featureSubsetStrategy = \"auto\"\n",
    "val impurity = \"gini\"\n",
    "val maxDepth = 15\n",
    "val maxBins = 32\n",
    "\n",
    "val startTime_dt=System.currentTimeMillis\n",
    "\n",
    "val model = RandomForest.trainClassifier(training, numClasses, categoricalFeaturesInfo,\n",
    "  numTrees, featureSubsetStrategy, impurity, maxDepth, maxBins)\n",
    "\n",
    "// Evaluate model on test instances and compute test error\n",
    "val labelAndPreds = test.map { point =>\n",
    "  val prediction = model.predict(point.features)\n",
    "  (point.label, prediction)\n",
    "}\n",
    "\n",
    "val endTime_dt=System.currentTimeMillis\n",
    "\n",
    "//Model Evaluation\n",
    "val metrics_dt = new MulticlassMetrics(labelAndPreds)\n",
    "val bMetrics_dt = new BinaryClassificationMetrics(labelAndPreds)\n",
    "println(\"Confusion Matrix of Random Forest is\")\n",
    "println(metrics_dt.confusionMatrix)\n",
    "println(\"\\nThe Accuracy of Random Forest is\")\n",
    "println(metrics_dt.accuracy)\n",
    "println(\"\\nThe ROC for Random Forest is\")\n",
    "println(bMetrics_dt.roc.collect().foreach(print))\n",
    "print(\"\\n\")\n",
    "println(\"\\nThe Area under ROC for Random Forest is\")\n",
    "println(bMetrics_dt.areaUnderROC)\n",
    "println(\"\\nThe Precision of Random Forest is\")\n",
    "println(metrics_dt.precision)\n",
    "println(\"\\nThe Precision by Threshold for Random Forest is\")\n",
    "println(bMetrics_dt.precisionByThreshold.collect().foreach(print))\n",
    "println(\"\\nExecution Time for Training and Prediction of Random Forest is\")\n",
    "println(endTime_dt-startTime_dt + \"ms\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike Decision Trees, Random Forest has bootstrapping aggragation and other techniques incorporated to the model itself for better data analysis. Just like it's data frame sibling RDD also has a very high execution time in comparison to other models.In fact it is the highest of all models considerd in this study. It is true that we have a set 500 trees while defining the model but that was for better average and accuracy. It is giving a high accuracy rate but we have a feeling that it is because of overfitting. 'Precision By Threshold' figures strengthened our doubts. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient-Boosted Tree\n",
    "----------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix of Gradient Boosted Tree is\n",
      "7404.0  37.0   \n",
      "48.0    294.0  \n",
      "\n",
      "The Accuracy of Gradient Boosted Tree is\n",
      "0.989078761403058\n",
      "\n",
      "The ROC for Gradient Boosted Tree is\n",
      "(0.0,0.0)(0.004972449939524257,0.8596491228070176)(1.0,1.0)(1.0,1.0)()\n",
      "\n",
      "The Area under ROC for Gradient Boosted Tree is\n",
      "0.9273383364337466\n",
      "\n",
      "The Precision of Gradient Boosted Tree is\n",
      "0.989078761403058\n",
      "\n",
      "The Precision by Threshold for Gradient Boosted Tree is\n",
      "(1.0,0.8882175226586103)(0.0,0.04394192470769626)()\n",
      "\n",
      "Execution Time for Training and Prediction of Gradient Boosted Tree is\n",
      "29491ms\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.mllib.tree.GradientBoostedTrees\n",
    "import org.apache.spark.mllib.tree.configuration.BoostingStrategy\n",
    "import org.apache.spark.mllib.tree.model.GradientBoostedTreesModel\n",
    "import org.apache.spark.mllib.util.MLUtils\n",
    "\n",
    "// Train a GradientBoostedTrees model.\n",
    "val boostingStrategy = BoostingStrategy.defaultParams(\"Classification\")\n",
    "boostingStrategy.numIterations = 20\n",
    "boostingStrategy.treeStrategy.numClasses = 2\n",
    "boostingStrategy.treeStrategy.maxDepth = 15\n",
    "boostingStrategy.treeStrategy.categoricalFeaturesInfo = Map[Int, Int]()\n",
    "boostingStrategy.learningRate=0.1\n",
    "\n",
    "val startTime_dt=System.currentTimeMillis\n",
    "\n",
    "val model = GradientBoostedTrees.train(training, boostingStrategy)\n",
    "\n",
    "// Evaluate model on test instances and compute test error\n",
    "val labelAndPreds = test.map { point =>\n",
    "  val prediction = model.predict(point.features)\n",
    "  (point.label, prediction)\n",
    "}\n",
    "\n",
    "val endTime_dt=System.currentTimeMillis\n",
    "\n",
    "//Model Evaluation\n",
    "val metrics_dt = new MulticlassMetrics(labelAndPreds)\n",
    "val bMetrics_dt = new BinaryClassificationMetrics(labelAndPreds)\n",
    "println(\"Confusion Matrix of Gradient Boosted Tree is\")\n",
    "println(metrics_dt.confusionMatrix)\n",
    "println(\"\\nThe Accuracy of Gradient Boosted Tree is\")\n",
    "println(metrics_dt.accuracy)\n",
    "println(\"\\nThe ROC for Gradient Boosted Tree is\")\n",
    "println(bMetrics_dt.roc.collect().foreach(print))\n",
    "print(\"\\n\")\n",
    "println(\"\\nThe Area under ROC for Gradient Boosted Tree is\")\n",
    "println(bMetrics_dt.areaUnderROC)\n",
    "println(\"\\nThe Precision of Gradient Boosted Tree is\")\n",
    "println(metrics_dt.precision)\n",
    "println(\"\\nThe Precision by Threshold for Gradient Boosted Tree is\")\n",
    "println(bMetrics_dt.precisionByThreshold.collect().foreach(print))\n",
    "println(\"\\nExecution Time for Training and Prediction of Gradient Boosted Tree is\")\n",
    "println(endTime_dt-startTime_dt + \"ms\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's was explicitly mentioned in spark documentation that Gradient-boosted tree of RDD gives inconsistent performance for imbalanced data. However setting 'learning rate' will help to reduce irregularities in performance and overfitting. So we have done just that for this analysis. Evaluations showed that though it is known a high performing model the Spark RDD version has limitation of handling imbalanced data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## CASE STUDY 3 CONCLUSION (FINAL CONCLUSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know that RDD are going to have an end of road map soon and Dataframes are the future. Our analysis revealed that models like Naive Bayes and SVM are inherently handling imbalanced data but these are not yet ported to data frames.Also there no development activities going on with RDD.In short the whole machine learning capabilities of spark has lot of limitations compared to python or R. This is one reason that spark is less popular in the market comapred to python or R. Inability to handle imbalanced data in many scenarios is a key drawback that we identified with this study.Our research also shows that development activities are in progress with spark dataframes to overcome many of this drawbacks. The JIRA link that we provided earlier in CASE STUDY 2 is strong proof for this. Hope we will see these features in spark soon and will make it more popular."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "file_extension": ".scala",
   "name": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
