{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.feature.{OneHotEncoder, StringIndexer,VectorAssembler}\n",
    "import org.apache.spark.sql.functions.udf\n",
    "import org.apache.spark.ml.Pipeline\n",
    "import org.apache.spark.sql.types.IntegerType\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.mllib.evaluation.MulticlassMetrics\n",
    "val sqlContext = new org.apache.spark.sql.SQLContext(sc)\n",
    "import sqlContext.implicits._\n",
    "import org.apache.spark.mllib.evaluation.BinaryClassificationMetrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Pre-Processing\n",
    "------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "val path = \"nassCDS.csv\"\n",
    "val readData = spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\").option(\"sep\", \",\").csv(path)\n",
    "val dataWithId  = readData.withColumnRenamed(\"_c0\",\"row_id\")\n",
    "val cleanData1 = dataWithId.na.drop\n",
    "val dvcatConvert = udf { (x: String) => \n",
    "    if (x == \"24-Oct\") \"10-24\"\n",
    "    else if (x == \"1-9km/h\" ) \"1-9\"\n",
    "    else x\n",
    "}\n",
    "val cleanData2 = cleanData1.withColumn(\"dvcatConverted\" , dvcatConvert(cleanData1(\"dvcat\")))\n",
    "val filterConvert = udf { (x: String) => \n",
    "    if (x == \"NA\") \"\"\n",
    "    else if (x != null ) x\n",
    "    else \"\"\n",
    "}\n",
    "val cleanData3 = cleanData2.withColumn(\"yearVehFilter\" , filterConvert(cleanData2(\"yearVeh\")))\n",
    "val cleanData4 = cleanData3.withColumn(\"injSeverityFilter\" , filterConvert(cleanData3(\"injSeverity\")))\n",
    "\n",
    "val cleanData5 = cleanData4.na.drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val dvcatIndexer = new StringIndexer().setInputCol(\"dvcatConverted\").setOutputCol(\"DvcatIndex\")\n",
    "val dvcatIndexed = dvcatIndexer.fit(cleanData5).transform(cleanData5)\n",
    "\n",
    "val deadIndexer = new StringIndexer().setInputCol(\"dead\").setOutputCol(\"label\")\n",
    "val deadIndexed = deadIndexer.fit(dvcatIndexed).transform(dvcatIndexed)\n",
    "\n",
    "val airbagInder = new StringIndexer().setInputCol(\"airbag\").setOutputCol(\"airbagIndex\")\n",
    "val airbagIndexed = airbagInder.fit(deadIndexed).transform(deadIndexed)\n",
    "\n",
    "val seatbeltIndexer = new StringIndexer().setInputCol(\"seatbelt\").setOutputCol(\"seatbeltIndex\")\n",
    "val seatbeltIndexed = seatbeltIndexer.fit(airbagIndexed).transform(airbagIndexed)\n",
    "\n",
    "val sexIndexer = new StringIndexer().setInputCol(\"sex\").setOutputCol(\"sexIndex\")\n",
    "val sexIndexed = sexIndexer.fit(seatbeltIndexed).transform(seatbeltIndexed)\n",
    "\n",
    "val abcatIndexer = new StringIndexer().setInputCol(\"abcat\").setOutputCol(\"abcatIndex\") \n",
    "val abcatIndexed = abcatIndexer.fit(sexIndexed).transform(sexIndexed)\n",
    "\n",
    "val occRoleIndexer = new StringIndexer().setInputCol(\"occRole\").setOutputCol(\"occRoleIndex\")\n",
    "val occRoleIndexed = occRoleIndexer.fit(abcatIndexed).transform(abcatIndexed)\n",
    "\n",
    "val yearVehIndexer = new StringIndexer().setInputCol(\"yearVeh\").setOutputCol(\"yearVehIndex\")\n",
    "val yearVehIndexed = yearVehIndexer.fit(occRoleIndexed).transform(occRoleIndexed)\n",
    "\n",
    "val injSeverityIndexer = new StringIndexer().setInputCol(\"injSeverity\").setOutputCol(\"injSeverityIndex\")\n",
    "val injSeverityIndexed = injSeverityIndexer.fit(yearVehIndexed).transform(yearVehIndexed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val dvcatEncoder = new OneHotEncoder().setInputCol(\"DvcatIndex\").setOutputCol(\"DvcatVec\")\n",
    "val dvcatEncoded = dvcatEncoder.transform(injSeverityIndexed)\n",
    "\n",
    "val airbagEncoder = new OneHotEncoder().setInputCol(\"airbagIndex\").setOutputCol(\"airbagVec\")\n",
    "val airbagEncoded = airbagEncoder.transform(dvcatEncoded)\n",
    "\n",
    "val seatbeltEncoder = new OneHotEncoder().setInputCol(\"seatbeltIndex\").setOutputCol(\"seatbeltVec\")\n",
    "val seatbeltEncoded = seatbeltEncoder.transform(airbagEncoded)\n",
    "\n",
    "val sexEncoder = new OneHotEncoder().setInputCol(\"sexIndex\").setOutputCol(\"sexVec\")\n",
    "val sexEncoded = sexEncoder.transform(seatbeltEncoded)\n",
    "\n",
    "val abcatEncoder = new OneHotEncoder().setInputCol(\"abcatIndex\").setOutputCol(\"abcatVec\")\n",
    "val abcatEncoded = abcatEncoder.transform(sexEncoded)\n",
    "\n",
    "val occRoleEncoder = new OneHotEncoder().setInputCol(\"occRoleIndex\").setOutputCol(\"occRoleVec\")\n",
    "val occRoleEncoded = occRoleEncoder.transform(abcatEncoded)\n",
    "\n",
    "val yearVehEncoder = new OneHotEncoder().setInputCol(\"yearVehIndex\").setOutputCol(\"yearVehVec\")\n",
    "val yearVehEncoded = yearVehEncoder.transform(occRoleEncoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val vecAssembler = new VectorAssembler().\n",
    "    setInputCols(Array(\"DvcatVec\",\"weight\",\"airbagVec\",\"seatbeltVec\",\"frontal\",\"sexVec\",\"ageOFocc\",\"yearacc\",\n",
    "    \"yearVehVec\",\"abcatVec\",\"occRoleVec\",\"deploy\",\"injSeverityIndex\")).\n",
    "    setOutputCol(\"features\")\n",
    "val vecAssembled = vecAssembler.transform(yearVehEncoded)\n",
    "val Array(train, test) = vecAssembled.randomSplit(Array(0.7, 0.3),seed=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression\n",
    "-------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.classification.LogisticRegression\n",
    "var startTime_lr = System.currentTimeMillis()\n",
    "val lr = new LogisticRegression().setLabelCol(\"label\").setFeaturesCol(\"features\").setMaxIter(10).setRegParam(0.3).setElasticNetParam(0.8)\n",
    "val predictionDF_lr = lr.fit(train).transform(test)\n",
    "var endTime_lr = System.currentTimeMillis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix of Logistics Regression is\n",
      "7477.0  0.0  \n",
      "338.0   0.0  \n",
      "\n",
      "The Accuracy of Logistics Regression is\n",
      "0.9567498400511836\n",
      "\n",
      "The ROC for Logistics Regression is\n",
      "(0.0,0.0)(1.0,1.0)(1.0,1.0)()\n",
      "\n",
      "The Area under ROC for Logistics Regression is\n",
      "0.5\n",
      "\n",
      "The Precision of Logistics Regression is\n",
      "0.9567498400511836\n",
      "\n",
      "The Precision by Threshold for Logistics Regression is\n",
      "(0.0,0.043250159948816376)()\n",
      "\n",
      "Execution Time for Training and Prediction of Logistics Regression is\n",
      "1579ms\n"
     ]
    }
   ],
   "source": [
    "//Model Evaluation\n",
    "val predictionAndLabels_MCM_lr = predictionDF_lr.select(\"prediction\",\"label\").as[(Double,Double)].rdd\n",
    "val metrics_lr = new MulticlassMetrics(predictionAndLabels_MCM_lr)\n",
    "val predictionLabelsRDD_lr = predictionDF_lr.select(\"prediction\", \"label\").map(r => (r.getDouble(0), r.getDouble(1)))\n",
    "val bMetrics_lr = new BinaryClassificationMetrics(predictionLabelsRDD_lr.rdd)\n",
    "println(\"Confusion Matrix of Logistics Regression is\")\n",
    "println(metrics_lr.confusionMatrix)\n",
    "println(\"\\nThe Accuracy of Logistics Regression is\")\n",
    "println(metrics_lr.accuracy)\n",
    "println(\"\\nThe ROC for Logistics Regression is\")\n",
    "println(bMetrics_lr.roc.collect().foreach(print))\n",
    "print(\"\\n\")\n",
    "println(\"\\nThe Area under ROC for Logistics Regression is\")\n",
    "println(bMetrics_lr.areaUnderROC)\n",
    "println(\"\\nThe Precision of Logistics Regression is\")\n",
    "println(metrics_lr.precision)\n",
    "println(\"\\nThe Precision by Threshold for Logistics Regression is\")\n",
    "println(bMetrics_lr.precisionByThreshold.collect().foreach(print))\n",
    "println(\"\\nExecution Time for Training and Prediction of Logistics Regression is\")\n",
    "println(endTime_lr-startTime_lr + \"ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initial impression with the above results were satisfactory under the assumption that it is possible to get such a high accuracy rate as we have only around 26000 records. A deeper analysis with confusion matrix, very low false positive rates and a really good ROC curve gave a feeling that it is actually over-fitting the data for some reason. Even after changing the seed value for train-test data split, the model was consistantly giving very high performance  and that strengthened the doubts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Decision Tree Classifier\n",
    "------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.classification.{DecisionTreeClassificationModel, DecisionTreeClassifier}\n",
    "var startTime_dt = System.currentTimeMillis()\n",
    "val dt = new DecisionTreeClassifier().setLabelCol(\"label\").setFeaturesCol(\"features\")\n",
    "val predictionDF_dt = dt.fit(train).transform(test)\n",
    "var endTime_dt = System.currentTimeMillis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix of Decision Tree is\n",
      "7471.0  6.0    \n",
      "30.0    308.0  \n",
      "\n",
      "The Accuracy of Decision Tree is\n",
      "0.9953934740882917\n",
      "\n",
      "The ROC for Decision Tree is\n",
      "(0.0,0.0)(8.024608800320985E-4,0.9112426035502958)(1.0,1.0)(1.0,1.0)()\n",
      "\n",
      "The Area under ROC for Decision Tree is\n",
      "0.9552200713351319\n",
      "\n",
      "The Precision of Decision Tree is\n",
      "0.9953934740882917\n",
      "\n",
      "The Precision by Threshold for Decision Tree is\n",
      "(1.0,0.9808917197452229)(0.0,0.043250159948816376)()\n",
      "\n",
      "Execution Time for Training and Prediction of Decision Tree is\n",
      "6882ms\n"
     ]
    }
   ],
   "source": [
    "//Model Evaluation\n",
    "val predictionAndLabels_MCM_dt = predictionDF_dt.select(\"prediction\",\"label\").as[(Double,Double)].rdd\n",
    "val metrics_dt = new MulticlassMetrics(predictionAndLabels_MCM_dt)\n",
    "val predictionLabelsRDD_dt = predictionDF_dt.select(\"prediction\", \"label\").map(r => (r.getDouble(0), r.getDouble(1)))\n",
    "val bMetrics_dt = new BinaryClassificationMetrics(predictionLabelsRDD_dt.rdd)\n",
    "println(\"Confusion Matrix of Decision Tree is\")\n",
    "println(metrics_dt.confusionMatrix)\n",
    "println(\"\\nThe Accuracy of Decision Tree is\")\n",
    "println(metrics_dt.accuracy)\n",
    "println(\"\\nThe ROC for Decision Tree is\")\n",
    "println(bMetrics_dt.roc.collect().foreach(print))\n",
    "print(\"\\n\")\n",
    "println(\"\\nThe Area under ROC for Decision Tree is\")\n",
    "println(bMetrics_dt.areaUnderROC)\n",
    "println(\"\\nThe Precision of Decision Tree is\")\n",
    "println(metrics_dt.precision)\n",
    "println(\"\\nThe Precision by Threshold for Decision Tree is\")\n",
    "println(bMetrics_dt.precisionByThreshold.collect().foreach(print))\n",
    "println(\"\\nExecution Time for Training and Prediction of Decision Tree is\")\n",
    "println(endTime_dt-startTime_dt + \"ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Trees are known for over-fitting the data and the the results above support that statement especially the accuracy rate of 99.5%. This is mainly because of that fact that decision Tree does not incorporate bootstrapping or any other technique to understand complete trend in the data set.It takes a random column as the root and then start building the decision tree from there. This is one reason for over-fitting. Multiple executions with different seed values for train-test split revealed that decision tree is over-fitting the data and also strengthened the doubts on the actual dataset itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest\n",
    "------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.classification.{RandomForestClassificationModel, RandomForestClassifier}\n",
    "var startTime_rf = System.currentTimeMillis()\n",
    "val rf = new RandomForestClassifier().setLabelCol(\"label\").setFeaturesCol(\"features\").setNumTrees(500)\n",
    "val predictionDF_rf = rf.fit(train).transform(test)\n",
    "var endTime_rf = System.currentTimeMillis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix of Random Forest is\n",
      "7473.0  4.0    \n",
      "46.0    292.0  \n",
      "\n",
      "The Accuracy of Random Forest is\n",
      "0.9936020473448497\n",
      "\n",
      "The ROC for Random Forest is\n",
      "(0.0,0.0)(5.349739200213989E-4,0.863905325443787)(1.0,1.0)(1.0,1.0)()\n",
      "\n",
      "The Area under ROC for Random Forest is\n",
      "0.9316851757618828\n",
      "\n",
      "The Precision of Random Forest is\n",
      "0.9936020473448497\n",
      "\n",
      "The Precision by Threshold for Random Forest is\n",
      "(1.0,0.9864864864864865)(0.0,0.043250159948816376)()\n",
      "\n",
      "The Model Execution Time for Random Forest is\n",
      "22949ms\n"
     ]
    }
   ],
   "source": [
    "//Model Evaluation\n",
    "val predictionAndLabels_MCM_rf = predictionDF_rf.select(\"prediction\",\"label\").as[(Double,Double)].rdd\n",
    "val metrics_rf = new MulticlassMetrics(predictionAndLabels_MCM_rf)\n",
    "val predictionLabelsRDD_rf = predictionDF_rf.select(\"prediction\", \"label\").map(r => (r.getDouble(0), r.getDouble(1)))\n",
    "val bMetrics_rf = new BinaryClassificationMetrics(predictionLabelsRDD_rf.rdd)\n",
    "println(\"Confusion Matrix of Random Forest is\")\n",
    "println(metrics_rf.confusionMatrix)\n",
    "println(\"\\nThe Accuracy of Random Forest is\")\n",
    "println(metrics_rf.accuracy)\n",
    "println(\"\\nThe ROC for Random Forest is\")\n",
    "println(bMetrics_rf.roc.collect().foreach(print))\n",
    "print(\"\\n\")\n",
    "println(\"\\nThe Area under ROC for Random Forest is\")\n",
    "println(bMetrics_rf.areaUnderROC)\n",
    "println(\"\\nThe Precision of Random Forest is\")\n",
    "println(metrics_rf.precision)\n",
    "println(\"\\nThe Precision by Threshold for Random Forest is\")\n",
    "println(bMetrics_rf.precisionByThreshold.collect().foreach(print))\n",
    "println(\"\\nExecution Time for Training and Prediction of Random Forest is\")\n",
    "println(endTime_rf-startTime_rf + \"ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike Decision Trees, Random Forest has bootstrapping aggragation and other techniques incorporated to the model itself for better performance. To get the best out of the model we have set the number of trees to 500 as well. This increased the execution time significantly compared to the logistic regression and decision tree. The accuracy was expecting to be high but the 'Precision by Threshold' evaluation paramater revealed that our dataset is having imbalanced data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient-boosted tree classifier\n",
    "--------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.classification.{GBTClassificationModel, GBTClassifier}\n",
    "var startTime_gbt = System.currentTimeMillis()\n",
    "val gbt = new GBTClassifier().setLabelCol(\"label\").setFeaturesCol(\"features\").setMaxIter(10)\n",
    "val predictionDF_gbt = gbt.fit(train).transform(test)\n",
    "var endTime_gbt = System.currentTimeMillis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix of Gradient-Boosted Tree is\n",
      "7471.0  6.0    \n",
      "29.0    309.0  \n",
      "\n",
      "The Accuracy of Gradient-Boosted Tree is\n",
      "0.9955214331413947\n",
      "\n",
      "The ROC for Gradient-Boosted Tree is\n",
      "(0.0,0.0)(8.024608800320985E-4,0.9142011834319527)(1.0,1.0)(1.0,1.0)()\n",
      "\n",
      "The Area under ROC for Gradient-Boosted Tree is\n",
      "0.9566993612759602\n",
      "\n",
      "The Precision of Gradient-Boosted Tree is\n",
      "0.9955214331413947\n",
      "\n",
      "The Precision by Threshold for Gradient-Boosted Tree is\n",
      "(1.0,0.9809523809523809)(0.0,0.043250159948816376)()\n",
      "\n",
      "Execution Time for Training and Prediction of Gradient-Boosted Tree is\n",
      "9647ms\n"
     ]
    }
   ],
   "source": [
    "//Model Evaluation\n",
    "val predictionAndLabels_MCM_gbt = predictionDF_gbt.select(\"prediction\",\"label\").as[(Double,Double)].rdd\n",
    "val metrics_gbt = new MulticlassMetrics(predictionAndLabels_MCM_gbt)\n",
    "val predictionLabelsRDD_gbt = predictionDF_gbt.select(\"prediction\", \"label\").map(r => (r.getDouble(0), r.getDouble(1)))\n",
    "val bMetrics_gbt = new BinaryClassificationMetrics(predictionLabelsRDD_gbt.rdd)\n",
    "println(\"Confusion Matrix of Gradient-Boosted Tree is\")\n",
    "println(metrics_gbt.confusionMatrix)\n",
    "println(\"\\nThe Accuracy of Gradient-Boosted Tree is\")\n",
    "println(metrics_gbt.accuracy)\n",
    "println(\"\\nThe ROC for Gradient-Boosted Tree is\")\n",
    "println(bMetrics_gbt.roc.collect().foreach(print))\n",
    "print(\"\\n\")\n",
    "println(\"\\nThe Area under ROC for Gradient-Boosted Tree is\")\n",
    "println(bMetrics_gbt.areaUnderROC)\n",
    "println(\"\\nThe Precision of Gradient-Boosted Tree is\")\n",
    "println(metrics_gbt.precision)\n",
    "println(\"\\nThe Precision by Threshold for Gradient-Boosted Tree is\")\n",
    "println(bMetrics_gbt.precisionByThreshold.collect().foreach(print))\n",
    "println(\"\\nExecution Time for Training and Prediction of Gradient-Boosted Tree is\")\n",
    "println(endTime_gbt-startTime_gbt + \"ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient-Boosted Tree which is considered as the best performing model of all the tree models and was expected to shine in model evaluation. But since we got an idea from Random Forest evaluation that we are dealing with an highly imbalanced dataset, the only parameter of interest in this evaluation was 'Precision By Threshold'. This again proved that our data is imbalanced and the evaluation parameters are not acceptable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CASE STUDY 1 CONCLUSION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It has been observed that there is a significant imbalance in data. A clear analysis on the dataset revealed that out of 26217 records available 25037 records states the person is 'alive' and only 1180 records corresponds to dead people. Training any model with this kind of data will not train the model effectively and most of the time the model will tend to predict the person is 'alive', which is happening in our scenario. So either we will have to under-sample the data or over-sample it. Since in our case under-sampling would reduce the number of records significantly, over-sampling will be the best approach. Alternately we can assign weights for the records as well to train the model. This leads us to CASE STUDY 2. It has also been observed that under default parameters Logistics regression's execution time is the fastest while Random Forest is the slowest. Gradient Boost Tree is a better choice in that regard too."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "file_extension": ".scala",
   "name": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
