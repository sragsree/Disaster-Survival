{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.feature.{OneHotEncoder, StringIndexer,VectorAssembler}\n",
    "import org.apache.spark.sql.functions.udf\n",
    "import org.apache.spark.ml.Pipeline\n",
    "import org.apache.spark.sql.types.IntegerType\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.mllib.evaluation.MulticlassMetrics\n",
    "val sqlContext = new org.apache.spark.sql.SQLContext(sc)\n",
    "import sqlContext.implicits._\n",
    "import org.apache.spark.mllib.evaluation.BinaryClassificationMetrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Pre-Processing\n",
    "------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "val path = \"nassCDS.csv\"\n",
    "val readData = spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\").option(\"sep\", \",\").csv(path)\n",
    "val dataWithId  = readData.withColumnRenamed(\"_c0\",\"row_id\")\n",
    "val cleanData1 = dataWithId.na.drop\n",
    "val dvcatConvert = udf { (x: String) => \n",
    "    if (x == \"24-Oct\") \"10-24\"\n",
    "    else if (x == \"1-9km/h\" ) \"1-9\"\n",
    "    else x\n",
    "}\n",
    "val cleanData2 = cleanData1.withColumn(\"dvcatConverted\" , dvcatConvert(cleanData1(\"dvcat\")))\n",
    "val filterConvert = udf { (x: String) => \n",
    "    if (x == \"NA\") \"\"\n",
    "    else if (x != null ) x\n",
    "    else \"\"\n",
    "}\n",
    "val cleanData3 = cleanData2.withColumn(\"yearVehFilter\" , filterConvert(cleanData2(\"yearVeh\")))\n",
    "val cleanData4 = cleanData3.withColumn(\"injSeverityFilter\" , filterConvert(cleanData3(\"injSeverity\")))\n",
    "\n",
    "val cleanData5 = cleanData4.na.drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val dvcatIndexer = new StringIndexer().setInputCol(\"dvcatConverted\").setOutputCol(\"DvcatIndex\")\n",
    "val dvcatIndexed = dvcatIndexer.fit(cleanData5).transform(cleanData5)\n",
    "\n",
    "val deadIndexer = new StringIndexer().setInputCol(\"dead\").setOutputCol(\"label\")\n",
    "val deadIndexed = deadIndexer.fit(dvcatIndexed).transform(dvcatIndexed)\n",
    "\n",
    "val airbagInder = new StringIndexer().setInputCol(\"airbag\").setOutputCol(\"airbagIndex\")\n",
    "val airbagIndexed = airbagInder.fit(deadIndexed).transform(deadIndexed)\n",
    "\n",
    "val seatbeltIndexer = new StringIndexer().setInputCol(\"seatbelt\").setOutputCol(\"seatbeltIndex\")\n",
    "val seatbeltIndexed = seatbeltIndexer.fit(airbagIndexed).transform(airbagIndexed)\n",
    "\n",
    "val sexIndexer = new StringIndexer().setInputCol(\"sex\").setOutputCol(\"sexIndex\")\n",
    "val sexIndexed = sexIndexer.fit(seatbeltIndexed).transform(seatbeltIndexed)\n",
    "\n",
    "val abcatIndexer = new StringIndexer().setInputCol(\"abcat\").setOutputCol(\"abcatIndex\") \n",
    "val abcatIndexed = abcatIndexer.fit(sexIndexed).transform(sexIndexed)\n",
    "\n",
    "val occRoleIndexer = new StringIndexer().setInputCol(\"occRole\").setOutputCol(\"occRoleIndex\")\n",
    "val occRoleIndexed = occRoleIndexer.fit(abcatIndexed).transform(abcatIndexed)\n",
    "\n",
    "val yearVehIndexer = new StringIndexer().setInputCol(\"yearVeh\").setOutputCol(\"yearVehIndex\")\n",
    "val yearVehIndexed = yearVehIndexer.fit(occRoleIndexed).transform(occRoleIndexed)\n",
    "\n",
    "val injSeverityIndexer = new StringIndexer().setInputCol(\"injSeverity\").setOutputCol(\"injSeverityIndex\")\n",
    "val injSeverityIndexed = injSeverityIndexer.fit(yearVehIndexed).transform(yearVehIndexed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val dvcatEncoder = new OneHotEncoder().setInputCol(\"DvcatIndex\").setOutputCol(\"DvcatVec\")\n",
    "val dvcatEncoded = dvcatEncoder.transform(injSeverityIndexed)\n",
    "\n",
    "val airbagEncoder = new OneHotEncoder().setInputCol(\"airbagIndex\").setOutputCol(\"airbagVec\")\n",
    "val airbagEncoded = airbagEncoder.transform(dvcatEncoded)\n",
    "\n",
    "val seatbeltEncoder = new OneHotEncoder().setInputCol(\"seatbeltIndex\").setOutputCol(\"seatbeltVec\")\n",
    "val seatbeltEncoded = seatbeltEncoder.transform(airbagEncoded)\n",
    "\n",
    "val sexEncoder = new OneHotEncoder().setInputCol(\"sexIndex\").setOutputCol(\"sexVec\")\n",
    "val sexEncoded = sexEncoder.transform(seatbeltEncoded)\n",
    "\n",
    "val abcatEncoder = new OneHotEncoder().setInputCol(\"abcatIndex\").setOutputCol(\"abcatVec\")\n",
    "val abcatEncoded = abcatEncoder.transform(sexEncoded)\n",
    "\n",
    "val occRoleEncoder = new OneHotEncoder().setInputCol(\"occRoleIndex\").setOutputCol(\"occRoleVec\")\n",
    "val occRoleEncoded = occRoleEncoder.transform(abcatEncoded)\n",
    "\n",
    "val yearVehEncoder = new OneHotEncoder().setInputCol(\"yearVehIndex\").setOutputCol(\"yearVehVec\")\n",
    "val yearVehEncoded = yearVehEncoder.transform(occRoleEncoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val vecAssembler = new VectorAssembler().\n",
    "    setInputCols(Array(\"DvcatVec\",\"weight\",\"airbagVec\",\"seatbeltVec\",\"frontal\",\"sexVec\",\"ageOFocc\",\"yearacc\",\n",
    "    \"yearVehVec\",\"abcatVec\",\"occRoleVec\",\"deploy\",\"injSeverityIndex\")).\n",
    "    setOutputCol(\"features\")\n",
    "val vecAssembled = vecAssembler.transform(yearVehEncoded)\n",
    "val Array(train, test) = vecAssembled.randomSplit(Array(0.7, 0.3),seed=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression (with Weighted Column) \n",
    "---------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "//Weighted (Over-Sampling)\n",
    "\n",
    "import org.apache.spark.ml.classification.LogisticRegression\n",
    "import org.apache.spark.sql.DataFrame\n",
    "def balanceDataset(dataset: DataFrame): DataFrame = {\n",
    "\n",
    "    // Re-balancing (weighting) of records to be used in the logistic loss objective function\n",
    "    val numNegatives = dataset.filter(dataset(\"label\") === 0).count\n",
    "    val datasetSize = dataset.count\n",
    "    val balancingRatio = (datasetSize - numNegatives).toDouble / datasetSize\n",
    "\n",
    "    val calculateWeights = udf { d: Double =>\n",
    "      if (d == 0.0) {\n",
    "        1 * balancingRatio\n",
    "      }\n",
    "      else {\n",
    "        (1 * (1.0 - balancingRatio))\n",
    "      }\n",
    "    }\n",
    "\n",
    "    val weightedDataset = dataset.withColumn(\"classWeightCol\", calculateWeights(dataset(\"label\")))\n",
    "    weightedDataset\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val weightedDF = balanceDataset(vecAssembled)\n",
    "val Array(train_w, test_w) = weightedDF.randomSplit(Array(0.7, 0.3),seed=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "var startTime_lr_w = System.currentTimeMillis()\n",
    "val lr_w = new LogisticRegression().setLabelCol(\"label\").setFeaturesCol(\"features\").setWeightCol(\"classWeightCol\").setMaxIter(10).setRegParam(0.3).setElasticNetParam(0.8)\n",
    "val predictionDF_lr_w = lr_w.fit(train_w).transform(test_w)\n",
    "var endTime_lr_w = System.currentTimeMillis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix of Logistics Regression is\n",
      "6117.0  1360.0  \n",
      "23.0    315.0   \n",
      "\n",
      "The Accuracy of Logistics Regression is\n",
      "0.8230326295585413\n",
      "\n",
      "The ROC for Logistics Regression is\n",
      "(0.0,0.0)(0.18189113280727565,0.9319526627218935)(1.0,1.0)(1.0,1.0)()\n",
      "\n",
      "The Area under ROC for Logistics Regression is\n",
      "0.8750307649573089\n",
      "\n",
      "The Precision of Logistics Regression is\n",
      "0.8230326295585413\n",
      "\n",
      "The Precision by Threshold for Logistics Regression is\n",
      "(1.0,0.1880597014925373)(0.0,0.043250159948816376)()\n",
      "\n",
      "Execution Time for Training and Prediction of Logistics Regression is\n",
      "4634ms\n"
     ]
    }
   ],
   "source": [
    "//Model Evaluation\n",
    "val predictionAndLabels_MCM_lr_w = predictionDF_lr_w.select(\"prediction\",\"label\").as[(Double,Double)].rdd\n",
    "val metrics_lr_w = new MulticlassMetrics(predictionAndLabels_MCM_lr_w)\n",
    "val predictionLabelsRDD_lr_w = predictionDF_lr_w.select(\"prediction\", \"label\").map(r => (r.getDouble(0), r.getDouble(1)))\n",
    "val bMetrics_lr_w = new BinaryClassificationMetrics(predictionLabelsRDD_lr_w.rdd)\n",
    "println(\"Confusion Matrix of Logistics Regression is\")\n",
    "println(metrics_lr_w.confusionMatrix)\n",
    "println(\"\\nThe Accuracy of Logistics Regression is\")\n",
    "println(metrics_lr_w.accuracy)\n",
    "println(\"\\nThe ROC for Logistics Regression is\")\n",
    "println(bMetrics_lr_w.roc.collect().foreach(print))\n",
    "print(\"\\n\")\n",
    "println(\"\\nThe Area under ROC for Logistics Regression is\")\n",
    "println(bMetrics_lr_w.areaUnderROC)\n",
    "println(\"\\nThe Precision of Logistics Regression is\")\n",
    "println(metrics_lr_w.precision)\n",
    "println(\"\\nThe Precision by Threshold for Logistics Regression is\")\n",
    "println(bMetrics_lr_w.precisionByThreshold.collect().foreach(print))\n",
    "println(\"\\nExecution Time for Training and Prediction of Logistics Regression is\")\n",
    "println(endTime_lr_w-startTime_lr_w + \"ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have used weighted column approach to overcome the unbalanced dataset issues. Now the evaluations results for logistical regression looks more realistic. It is expected that logistic regression to do good with categorical variable and classification and 82.3 is not a bad figure for accuracy in that regard.But one thing to note here is that the execution time for logistical regression has been increased compared to the one in CASE STUDY 1. It might be because we are using a weighted column approach. It is the best approach for logistical regression to deal with unbalanced data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Tree (With Defined thresholds)\n",
    "------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.classification.DecisionTreeClassificationModel\n",
    "import org.apache.spark.ml.classification.DecisionTreeClassifier\n",
    "var startTime_dt_w = System.currentTimeMillis()\n",
    "val dt_w = new DecisionTreeClassifier().setLabelCol(\"label\").setFeaturesCol(\"features\").setThresholds(Array(0.95,0.05))\n",
    "val predictionDF_dt_w = dt_w.fit(train).transform(test)\n",
    "var endTime_dt_w = System.currentTimeMillis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix of Decision Tree is\n",
      "7331.0  146.0  \n",
      "17.0    321.0  \n",
      "\n",
      "The Accuracy of Decision Tree is\n",
      "0.9791426743442099\n",
      "\n",
      "The ROC for Decision Tree is\n",
      "(0.0,0.0)(0.019526548080781064,0.9497041420118343)(1.0,1.0)(1.0,1.0)()\n",
      "\n",
      "The Area under ROC for Decision Tree is\n",
      "0.9650887969655266\n",
      "\n",
      "The Precision of Decision Tree is\n",
      "0.9791426743442099\n",
      "\n",
      "The Precision by Threshold for Decision Tree is\n",
      "(1.0,0.6873661670235546)(0.0,0.043250159948816376)()\n",
      "\n",
      "Execution Time for Training and Prediction of Decision Tree is\n",
      "4730ms\n"
     ]
    }
   ],
   "source": [
    "//Model Evaluation\n",
    "val predictionAndLabels_MCM_dt_w = predictionDF_dt_w.select(\"prediction\",\"label\").as[(Double,Double)].rdd\n",
    "val metrics_dt_w = new MulticlassMetrics(predictionAndLabels_MCM_dt_w)\n",
    "val predictionLabelsRDD_dt_w = predictionDF_dt_w.select(\"prediction\", \"label\").map(r => (r.getDouble(0), r.getDouble(1)))\n",
    "val bMetrics_dt_w = new BinaryClassificationMetrics(predictionLabelsRDD_dt_w.rdd)\n",
    "println(\"Confusion Matrix of Decision Tree is\")\n",
    "println(metrics_dt_w.confusionMatrix)\n",
    "println(\"\\nThe Accuracy of Decision Tree is\")\n",
    "println(metrics_dt_w.accuracy)\n",
    "println(\"\\nThe ROC for Decision Tree is\")\n",
    "println(bMetrics_dt_w.roc.collect().foreach(print))\n",
    "print(\"\\n\")\n",
    "println(\"\\nThe Area under ROC for Decision Tree is\")\n",
    "println(bMetrics_dt_w.areaUnderROC)\n",
    "println(\"\\nThe Precision of Decision Tree is\")\n",
    "println(metrics_dt_w.precision)\n",
    "println(\"\\nThe Precision by Threshold for Decision Tree is\")\n",
    "println(bMetrics_dt_w.precisionByThreshold.collect().foreach(print))\n",
    "println(\"\\nExecution Time for Training and Prediction of Decision Tree is\")\n",
    "println(endTime_dt_w-startTime_dt_w + \"ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here to overcome the umbalanced data problem we are setting the threshold values while defining the model. The weighted column approach is not yet available in decision trees. Setting thresholds is not a recommended approach in other languages but in spark this is the best solution currently availabe to deal with unbalanced data. 'Precision by Threshold' is almost balanced in this approach and that is the proof that we are getting a realistic accuracy rate. Changing the seed value and repeatedly executing the model was giving accuracy around 95% in some scenarios and an even more balanced 'Precision by Threshold' values. As decision trees are known for over-fitting we cannot relay much on this model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest (with Defined Thresholds)\n",
    "------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.classification.{RandomForestClassificationModel, RandomForestClassifier}\n",
    "var startTime_rf_w = System.currentTimeMillis()\n",
    "val rf_w = new RandomForestClassifier().setLabelCol(\"label\").setFeaturesCol(\"features\").setNumTrees(500).setThresholds(Array(0.95,0.05))\n",
    "val predictionDF_rf_w = rf_w.fit(train).transform(test)\n",
    "var endTime_rf_w = System.currentTimeMillis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix of Random Forest is\n",
      "6708.0  769.0  \n",
      "11.0    327.0  \n",
      "\n",
      "The Accuracy of Random Forest is\n",
      "0.9001919385796545\n",
      "\n",
      "The ROC for Random Forest is\n",
      "(0.0,0.0)(0.10284873612411395,0.9674556213017751)(1.0,1.0)(1.0,1.0)()\n",
      "\n",
      "The Area under ROC for Random Forest is\n",
      "0.9323034425888306\n",
      "\n",
      "The Precision of Random Forest is\n",
      "0.9001919385796545\n",
      "\n",
      "The Precision by Threshold for Random Forest is\n",
      "(1.0,0.2983576642335766)(0.0,0.043250159948816376)()\n",
      "\n",
      "Execution Time for Training and Prediction of Random Forest is\n",
      "22098ms\n"
     ]
    }
   ],
   "source": [
    "//Model Evaluation\n",
    "val predictionAndLabels_MCM_rf_w = predictionDF_rf_w.select(\"prediction\",\"label\").as[(Double,Double)].rdd\n",
    "val metrics_rf_w = new MulticlassMetrics(predictionAndLabels_MCM_rf_w)\n",
    "val predictionLabelsRDD_rf_w = predictionDF_rf_w.select(\"prediction\", \"label\").map(r => (r.getDouble(0), r.getDouble(1)))\n",
    "val bMetrics_rf_w = new BinaryClassificationMetrics(predictionLabelsRDD_rf_w.rdd)\n",
    "println(\"Confusion Matrix of Random Forest is\")\n",
    "println(metrics_rf_w.confusionMatrix)\n",
    "println(\"\\nThe Accuracy of Random Forest is\")\n",
    "println(metrics_rf_w.accuracy)\n",
    "println(\"\\nThe ROC for Random Forest is\")\n",
    "println(bMetrics_rf_w.roc.collect().foreach(print))\n",
    "print(\"\\n\")\n",
    "println(\"\\nThe Area under ROC for Random Forest is\")\n",
    "println(bMetrics_rf_w.areaUnderROC)\n",
    "println(\"\\nThe Precision of Random Forest is\")\n",
    "println(metrics_rf_w.precision)\n",
    "println(\"\\nThe Precision by Threshold for Random Forest is\")\n",
    "println(bMetrics_rf_w.precisionByThreshold.collect().foreach(print))\n",
    "println(\"\\nExecution Time for Training and Prediction of Random Forest is\")\n",
    "println(endTime_rf_w-startTime_rf_w + \"ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here also to deal with unbalanced data we are setting the thresholds while defining the model. Just as in decison tree Random Forest don't have weighted column approach at this point of time however this feature is in development phase now(Proof: https://issues.apache.org/jira/browse/SPARK-9478 ). Here we are getting a better 'precision by threshold' balance. The accuracy however has been reduced. This might be beacause of the fact that decision tree is over-fitting the data and when it comes to Random Forest because of bootstraping and other features are build-in, it is able to analyse the data in a better way. For this reason Random Forest is a more reliable model than decision tree. Another important thing to not here is that even after incorporating threshold into the model there is not much change in execution time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient-boosted tree classifier (With Defined Thresholds)\n",
    "------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.classification.{GBTClassificationModel, GBTClassifier}\n",
    "var startTime_gbt_w = System.currentTimeMillis()\n",
    "val gbt_w = new GBTClassifier().setLabelCol(\"label\").setFeaturesCol(\"features\").setMaxIter(10).setThresholds(Array(0.90,0.10))\n",
    "val predictionDF_gbt_w = gbt_w.fit(train).transform(test)\n",
    "var endTime_gbt_w = System.currentTimeMillis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix of Gradient-Boosted Tree is\n",
      "7430.0  47.0   \n",
      "23.0    315.0  \n",
      "\n",
      "The Accuracy of Gradient-Boosted Tree is\n",
      "0.9910428662827895\n",
      "\n",
      "The ROC for Gradient-Boosted Tree is\n",
      "(0.0,0.0)(0.006285943560251438,0.9319526627218935)(1.0,1.0)(1.0,1.0)()\n",
      "\n",
      "The Area under ROC for Gradient-Boosted Tree is\n",
      "0.962833359580821\n",
      "\n",
      "The Precision of Gradient-Boosted Tree is\n",
      "0.9910428662827895\n",
      "\n",
      "The Precision by Threshold for Gradient-Boosted Tree is\n",
      "(1.0,0.8701657458563536)(0.0,0.043250159948816376)()\n",
      "\n",
      "Execution Time for Training and Prediction of Gradient-Boosted Tree is\n",
      "7213ms\n"
     ]
    }
   ],
   "source": [
    "//Model Evaluation\n",
    "val predictionAndLabels_MCM_gbt_w = predictionDF_gbt_w.select(\"prediction\",\"label\").as[(Double,Double)].rdd\n",
    "val metrics_gbt_w = new MulticlassMetrics(predictionAndLabels_MCM_gbt_w)\n",
    "val predictionLabelsRDD_gbt_w = predictionDF_gbt_w.select(\"prediction\", \"label\").map(r => (r.getDouble(0), r.getDouble(1)))\n",
    "val bMetrics_gbt_w = new BinaryClassificationMetrics(predictionLabelsRDD_gbt_w.rdd)\n",
    "println(\"Confusion Matrix of Gradient-Boosted Tree is\")\n",
    "println(metrics_gbt_w.confusionMatrix)\n",
    "println(\"\\nThe Accuracy of Gradient-Boosted Tree is\")\n",
    "println(metrics_gbt_w.accuracy)\n",
    "println(\"\\nThe ROC for Gradient-Boosted Tree is\")\n",
    "println(bMetrics_gbt_w.roc.collect().foreach(print))\n",
    "print(\"\\n\")\n",
    "println(\"\\nThe Area under ROC for Gradient-Boosted Tree is\")\n",
    "println(bMetrics_gbt_w.areaUnderROC)\n",
    "println(\"\\nThe Precision of Gradient-Boosted Tree is\")\n",
    "println(metrics_gbt_w.precision)\n",
    "println(\"\\nThe Precision by Threshold for Gradient-Boosted Tree is\")\n",
    "println(bMetrics_gbt_w.precisionByThreshold.collect().foreach(print))\n",
    "println(\"\\nExecution Time for Training and Prediction of Gradient-Boosted Tree is\")\n",
    "println(endTime_gbt_w-startTime_gbt_w + \"ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Since Spark dataframe is an evolving technology especially for machine learning libraries, there is not much reference over the internet on in Spark official document on how to effectively handle imbalanced dataset. For this study we followed the same approach we used for decison tree and random forest as all of them are tree based model. However it didn't make much difference in the evaluation results, It does changed the 'Precision by threshold' results by a small margin but not much.The execution time for Gradient Booosted Tree without and with defined threshold were observed to be almost same. Gradient Boosted Trees are expected to give the best results but in this special case the though the results are good the question is how reliable they are? We feel it's not handling the imbalanced data effectively. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CASE STUDY 2 CONCLUSION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our attempts to handle umbalanced data in spark models itself made us explore different routes. Weighted Column is the approach that spark has taken to handle this kind of scenarios. As of now its been implemented only for logistic regression. For tree based models this feature is in development phase and hence we have taking the approach of setting thresholds on the models explicitly. Though this is not a recommended approach in python or R, In spark this is the best possible solution available. We were a bit curious to see how RDD handled this scenario in spark and that led us to CASE STUDY 3."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "file_extension": ".scala",
   "name": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
